{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from Bio import Entrez\n",
    "import redis\n",
    "import redis\n",
    "import hashlib\n",
    "import re\n",
    "from ftplib import FTP \n",
    "import codecs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Entrez.email = \"yunfang@chanzuckerberg.com\"\n",
    "redis_store = redis.StrictRedis(host='localhost', port=6379, db=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SR_PREFIX = 'sr:pg:'\n",
    "GDS_PREFIX = 'gds:'\n",
    "SRA_PREFIX = 'sra:'\n",
    "PAGE_SIZE = 500\n",
    "ITEMS_PER_DOWNLOAD = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def query_to_sig(query):\n",
    "    return hashlib.md5(query).hexdigest()\n",
    "    \n",
    "def get_search_results_from_gds(queries):\n",
    "    '''Getting list of GDS that matches our search query'''\n",
    "    output_summary = {}\n",
    "    for query in queries:\n",
    "        sig = query_to_sig(query)\n",
    "        page = 0\n",
    "        page_size = PAGE_SIZE\n",
    "        while page_size >= PAGE_SIZE:\n",
    "            try:\n",
    "                print (\"Downloading page %d for query '%s'\" % (page, query))\n",
    "                handle = Entrez.esearch(db=\"gds\", retmax=PAGE_SIZE, retstart=page*PAGE_SIZE, term=query)\n",
    "                record = Entrez.read(handle)\n",
    "                handle.close()\n",
    "                json_str = json.dumps(record['IdList'])\n",
    "                redis_key = SR_PREFIX + sig + ':' + str(page)\n",
    "                redis_store.set(redis_key, json_str)\n",
    "                page_size = len(record['IdList'])\n",
    "                page+=1\n",
    "                time.sleep(1)\n",
    "            except Exception:\n",
    "                print(\"Error getting page %d. Retry in 5 minutes ....\" % page)\n",
    "                time.sleep(5)\n",
    "        output_summary[sig] = page\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading page 0 for query 'single cell rnaseq'\n",
      "Downloading page 1 for query 'single cell rnaseq'\n",
      "Downloading page 2 for query 'single cell rnaseq'\n",
      "Downloading page 3 for query 'single cell rnaseq'\n"
     ]
    }
   ],
   "source": [
    "#id_lists = get_search_results_from_gds(['single cell rna', 'single cell rnaseq','single-cell rnaseq'])\n",
    "id_list = get_search_results_from_gds(['single cell rnaseq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fetch_esummary(id_str, mydb='gds', pfx = GDS_PREFIX):\n",
    "    while True:\n",
    "        try:\n",
    "            handle = Entrez.esummary(db=mydb, id=id_str)\n",
    "            records = Entrez.read(handle)\n",
    "            handle.close()\n",
    "            \n",
    "            for r in records: \n",
    "                key = \"%s%s\" %(pfx, r['Id'])\n",
    "                val = json.dumps(r)\n",
    "                redis_store.set(key, val)\n",
    "            return\n",
    "        except Exception:\n",
    "            print(\"ERROR getting %s\" % id_str)\n",
    "            time.sleep(5)\n",
    "            \n",
    "def download_search_results(query_signature, force_download = False):\n",
    "    page = 0\n",
    "    while 1:\n",
    "        print \"Downloading page %d for %s\" % (page, query_signature)\n",
    "        redis_key = SR_PREFIX + query_signature + ':' + str(page) \n",
    "        res = redis_store.get(redis_key) \n",
    "        if res is None:\n",
    "            return\n",
    "        id_list = json.loads(res)\n",
    "        batch_list = []\n",
    "        for doc_id in id_list:\n",
    "            if len(batch_list) >= ITEMS_PER_DOWNLOAD:\n",
    "                #print (\"  idx: %d key: %s\" % (idx, id_str))\n",
    "                fetch_esummary(\",\".join(batch_list), 'gds', GDS_PREFIX)\n",
    "                time.sleep(1)\n",
    "                batch_list = []\n",
    "            if force_download or redis_store.get(GDS_PREFIX+doc_id) is None:\n",
    "                batch_list.append(doc_id)                \n",
    "        #print (\"  idx: %d key: %s\" % (idx, id_str))\n",
    "        if len(batch_list) > 0:\n",
    "            fetch_esummary(\",\".join(batch_list), 'gds', GDS_PREFIX)\n",
    "        page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading page 0 for 56a5539da463486bd62ad1f27e396dc0\n",
      "Downloading page 1 for 56a5539da463486bd62ad1f27e396dc0\n",
      "Downloading page 2 for 56a5539da463486bd62ad1f27e396dc0\n",
      "Downloading page 3 for 56a5539da463486bd62ad1f27e396dc0\n",
      "Downloading page 4 for 56a5539da463486bd62ad1f27e396dc0\n"
     ]
    }
   ],
   "source": [
    "sig = query_to_sig('single cell rnaseq')\n",
    "download_search_results(sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc_re = re.compile(\"single[\\s\\-]*cell\", re.M)\n",
    "def doc_filter(doc_hash):\n",
    "    if not doc_hash['ExtRelations']:\n",
    "        return False\n",
    "\n",
    "    summary = doc_hash['summary'].lower() \n",
    "    title = doc_hash['title'].lower()\n",
    "    if not sc_re.match(summary) and not sc_re.match(title):\n",
    "        return False\n",
    "    \n",
    "    if not doc_hash['PubMedIds']:\n",
    "        return False\n",
    "                      \n",
    "    return True\n",
    "    \n",
    "    \n",
    "def get_final_sra_list(queries):\n",
    "    final_list = []\n",
    "    for query in queries:\n",
    "        sig = query_to_sig(query)\n",
    "        page = 0\n",
    "        while True:\n",
    "            redis_key = SR_PREFIX + sig + ':' + str(page) \n",
    "            res = redis_store.get(redis_key) \n",
    "            if res is None:\n",
    "                break\n",
    "            id_list = json.loads(res)\n",
    "            for doc_id in id_list:\n",
    "                doc_hash = json.loads(redis_store.get(GDS_PREFIX+doc_id))\n",
    "                if doc_filter(doc_hash):\n",
    "                    final_list.append(doc_id)\n",
    "            page += 1\n",
    "    return list(set(final_list))\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_download_list = get_final_sra_list(['single cell rnaseq', 'single-cell rnaseq'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_file_list_from_ftp(starting_dir, ftp):\n",
    "    ftp.cwd(starting_dir)\n",
    "    flist = ftp.nlst(\"*/*\")\n",
    "    results = []\n",
    "    for f in flist:\n",
    "        results.append(['-', starting_dir + '/' + f, 0])\n",
    "    return results\n",
    "    \n",
    "def get_sra_files(doc_id):\n",
    "    redis_key = GDS_PREFIX + doc_id\n",
    "    rec = json.loads(redis_store.get(redis_key))\n",
    "    sra_list = rec['ExtRelations']\n",
    "    result_list = []\n",
    "    idx = 0\n",
    "    tries = 0\n",
    "    while idx < len(sra_list):\n",
    "        s = sra_list[idx]\n",
    "        if s[\"RelationType\"] != 'SRA':\n",
    "            continue\n",
    "        sra_url = s[\"TargetFTPLink\"]\n",
    "        m = re.match(\"ftp\\:\\/\\/([^\\/]+)(\\/.*)\", sra_url)\n",
    "        try:\n",
    "            ftp_host = m.group(1)\n",
    "            ftp_dir  = m.group(2)\n",
    "            ftph = FTP(ftp_host)\n",
    "            ftph.login()\n",
    "            result_list = result_list + get_file_list_from_ftp(ftp_dir, ftph)\n",
    "            ftph.close()\n",
    "            tries = 0\n",
    "            idx += 1\n",
    "        except Exception:\n",
    "            tries += 1\n",
    "            if tries < 3:\n",
    "                print(\"ERROR getting %s. Retry in 5 seconds...\" % sra_url)\n",
    "                time.sleep(5)\n",
    "            else:\n",
    "                idx += 1 # failed too many times. move on to the next one. \n",
    "                tries = 0\n",
    "        \n",
    "    return result_list\n",
    "\n",
    "\n",
    "\n",
    "def download_sra_urls(doc_list, force_download = False):\n",
    "    for doc_id in doc_list:\n",
    "        print(\"=======Downloading SRA for doc %s ====\" % doc_id)\n",
    "        redis_key = SRA_PREFIX + doc_id\n",
    "        if force_download or redis_store.get(redis_key) is None:\n",
    "            res = get_sra_files(doc_id)\n",
    "            redis_val = json.dumps(res)\n",
    "            redis_store.set(redis_key, redis_val)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def doc_list_to_file(doc_list, filename):\n",
    "    with open(filename, 'w') as fd:\n",
    "        for doc_id in doc_list:\n",
    "            fd.write(\"%s\\n\" % doc_id)            \n",
    "def doc_summary_to_file(doc_list, filename):\n",
    "    with codecs.open(filename,'w', encoding='utf-8') as fd:\n",
    "        idx = 1\n",
    "        for doc_id in doc_list:\n",
    "            redis_key = 'gds:' + doc_id\n",
    "            rec = json.loads(redis_store.get(redis_key))               \n",
    "            pubmed_str = \",\".join([str(x) for x in rec['PubMedIds']])\n",
    "            output = \"%d. Doc ID: %s, GSE: %s: PubMed Id: %s, Taxon: %s, Date: %s, Samples: %s\\n\" % \\\n",
    "                (idx, doc_id, str(rec['GSE']), pubmed_str, rec['taxon'], rec['PDAT'], str(rec['n_samples'])) \n",
    "    \n",
    "            output += \"Title: %s \\n\" % rec['title']\n",
    "            output += \"Summary: %s \\n\" % rec['summary']\n",
    "            output += \"# SRA URLS: %d \\n\" % len(rec['ExtRelations'])\n",
    "            output += \"=============================================================\\n\"\n",
    "            fd.write(\"%s\\n\" % output)\n",
    "            idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======Downloading SRA for doc 200075386 ====\n",
      "=======Downloading SRA for doc 200064002 ====\n",
      "=======Downloading SRA for doc 200084789 ====\n",
      "=======Downloading SRA for doc 200083948 ====\n",
      "=======Downloading SRA for doc 200083146 ====\n",
      "=======Downloading SRA for doc 200067833 ====\n",
      "=======Downloading SRA for doc 200079510 ====\n",
      "=======Downloading SRA for doc 200080032 ====\n"
     ]
    }
   ],
   "source": [
    "download_sra_urls(final_download_list)\n",
    "doc_list_to_file(final_download_list, 'sra_doc_ids.txt')\n",
    "doc_summary_to_file(final_download_list, 'sra_doc_summaries.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
